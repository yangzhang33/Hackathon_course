{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cleaning and exploratory analysis\n",
    "\n",
    "Authors : Haddam Yacine, Ka Alioune, Renaud Adrien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"./figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning\n",
    "======\n",
    "\n",
    "#### How can it be problematic for our analyst to use the dataset as is, without cleaning? \n",
    "\n",
    "#### WHAT IS DATA CLEANING:\n",
    "The purpose of this step is to normalize the data to facilitate its manipulation during the analysis.\n",
    "Several operations are possible: modify or delete data that are incorrect, incomplete, irrelevant, corrupted, duplicated or badly formatted\n",
    "\n",
    "\n",
    "### Why is this important?Â \n",
    "- Correct duplicate or misfiled data. \n",
    "- Correct errors in manual data entry. \n",
    "- Wrong data can affect the results and their accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective of this lab\n",
    "======\n",
    "\n",
    "\n",
    "Clean the datasets in order to obtain a quality dataset, without errors, duplicates, irrelevant values... ready to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Path\n",
    "\n",
    "`data_dir` is the path to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jovyan/personal_workspace/bootcamp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "sys.path.append('../src/notebooks')\n",
    "from utils.get_data import load_data\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Metadata \n",
    "\n",
    "The first file contains a description of the buildings :\n",
    "\n",
    "* `building_id`: unique identifier for the building.\n",
    "* `site_id`: unique identifier for the site (multiple buildings are located at the same site).\n",
    "* `primary_use`: Primary space usage of all buildings is mapped using the [energystar scheme building description types](https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/identify-your-property-type). \n",
    "* `\tsub_primary_use`: [energystar scheme building description types](https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/identify-your-property-type) subcategory.\n",
    "* `square_feet`:Floor area of building in square meters (m2).\n",
    "* `lat`: Latitude of building location to site level.\n",
    "* `lng`: Longitude of building location to site level.\n",
    "* `year_built`: Year corresponding to when building was first constructed, in the format YYYY.\n",
    "* `floor_count`: Number of floors corresponding to building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta = pd.read_feather(\n",
    "    os.path.join(data_dir, 'raw/building_metadata.feather')\n",
    ")\n",
    "building_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many rows and columns contains the building_metadata file ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_rows = building_meta.shape[0]\n",
    "numbers_columns = building_meta.shape[1]\n",
    "print(f'Building metadata rows : {numbers_rows} | columns : {numbers_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the percentage of empty value in each column ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na values\n",
    "print('The perc. of empty values')\n",
    "for column_name in building_meta:\n",
    "    percentage_na = building_meta[column_name].isna().mean() * 100\n",
    "    print(f'{column_name} : {np.round(percentage_na)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the two column 'year_build' and 'floor_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with high percentage of na values\n",
    "building_meta = building_meta.drop(columns=['year_built', 'floor_count'])\n",
    "building_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14% of building_id have an NaN (Not a Numerical) values -> drop this rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the rows with NaN Lat and Lng\n",
    "# | : or logique\n",
    "mask_lat_lng_na = (\n",
    "    building_meta.lat.isna().values\n",
    "    | building_meta.lng.isna().values\n",
    ")\n",
    "\n",
    "# ~: tilde sign identify a not logique\n",
    "# filter rows with mmask_lat_lng_na\n",
    "building_meta = building_meta[~mask_lat_lng_na].reset_index(drop=True)\n",
    "\n",
    "numbers_rows = building_meta.shape[0]\n",
    "numbers_columns = building_meta.shape[1]\n",
    "print(f'Building metadata without na rows rows: {numbers_rows} | columns : {numbers_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a duplicates rows ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_duplicated = building_meta.duplicated(keep=False)\n",
    "building_meta[mask_duplicated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop all duplicates rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta = building_meta.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "numbers_rows = building_meta.shape[0]\n",
    "numbers_columns = building_meta.shape[1]\n",
    "print(f'Building metadata without duplicates rows : {numbers_rows} | columns : {numbers_columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many unique building and site are presented ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_unique_building = building_meta.building_id.nunique()\n",
    "number_unique_site = building_meta.site_id.nunique()\n",
    "print(f'Number of unique building {number_unique_building}')\n",
    "print(f'Number of unique site {number_unique_site}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the percentage of building in each category of primary_use ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_bulding_per_primary_use = (\n",
    "    building_meta\n",
    "    .groupby('primary_use')\n",
    "    ['building_id']\n",
    "    .agg(\"nunique\") \n",
    "    / number_unique_building\n",
    "    * 100\n",
    ")\n",
    "perc_bulding_per_primary_use.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice categories with low percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta.primary_use.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grouped them into homogenous categories to facilite the data analysis :\n",
    "- 'Warehouse/storage', 'Manufacturing/industrial','Technology/science', 'Utility' --> 'Industry'\n",
    "- 'Religious worship' --> 'Other'\n",
    "- 'Retail', 'Food sales and service' --> Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maping_primary_use_grouped = {\n",
    "    'Education': 'Education',\n",
    "    'Lodging/residential': 'Lodging/residential',\n",
    "    'Office': 'Office',\n",
    "    'Entertainment/public assembly': 'Entertainment/public assembly',\n",
    "    'Other': 'Other',\n",
    "    'Retail': 'Services',\n",
    "    'Parking': 'Parking',\n",
    "    'Public services': 'Public services',\n",
    "    'Warehouse/storage': 'Industry',\n",
    "    'Food sales and service': 'Services',\n",
    "    'Religious worship': 'Other',\n",
    "    'Healthcare': 'Healthcare',\n",
    "    'Utility': 'Industry',\n",
    "    'Technology/science': 'Industry',\n",
    "    'Manufacturing/industrial': 'Industry',\n",
    "    'Services': 'Services'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta.primary_use = (\n",
    "    building_meta\n",
    "    .primary_use\n",
    "    .map(maping_primary_use_grouped)\n",
    ")\n",
    "perc_bulding_per_primary_use = (\n",
    "    building_meta\n",
    "    .groupby('primary_use')\n",
    "    ['building_id']\n",
    "    .agg(\"nunique\")\n",
    "    / number_unique_building\n",
    "    * 100\n",
    ")\n",
    "perc_bulding_per_primary_use.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to verify the quality of geographic data ?\n",
    "\n",
    "#### Easy ! A map plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_position = (\n",
    "    building_meta\n",
    "    [['site_id', 'lat', 'lng']]\n",
    "    .drop_duplicates()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    site_position, lat=\"lat\", lon=\"lng\", hover_name=\"site_id\",\n",
    "    color_discrete_sequence=[\"fuchsia\"], zoom=2, height=300\n",
    ")\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see an false coordinate fo the building_id 16 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exclude all rows with the building_id == 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta = building_meta[building_meta.site_id != 16].reset_index(drop=True)\n",
    "building_meta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about square_feet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul statistics to check continuous variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get statictis aof the variable\n",
    "building_meta.square_feet.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot a distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.displot(building_meta, x='square_feet', kde=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't notice any negative values or incoherent distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result of cleaning building_metadata in the folder clean to use for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_meta.to_feather(\n",
    "    os.path.join(data_dir, 'clean/building_metadata.feather')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weather\n",
    "\n",
    "The second file contains weather data in 2016 and 2017 (every hour) for all the `site_id` referenced in `building_metadata`.\n",
    "\n",
    "* <code>timestamp</code>: date and time in the format YYYY-MM-DD hh:mm:ss. Local timezone.\n",
    "* <code>site_id</code>: unique identifier for the site.\n",
    "* <code>air_temperature</code>: The temperature of the air in degrees Celsius (ÂºC).\n",
    "* <code>cloud_coverage</code>: Portion of the sky covered in clouds, in [oktas](https://en.wikipedia.org/wiki/Okta).\n",
    "* <code>dew_temperature</code>: The dew point (the temperature to which a given parcel of air must be cooled at constant pressure and water vapor content in order for saturation to occur) in degrees Celsius (ÂºC).\n",
    "* <code>precip_depth_1_hr</code>: The depth of liquid precipitation that is measured over a one hour accumulation period (mm).\n",
    "* <code>sea_lvl_pressure</code>: The air pressure relative to Mean Sea Level (MSL) (mbar or hPa).\n",
    "* <code>wind_direction</code>: The angle, measured in a clockwise direction, between true north and the direction from which the wind is blowing (degrees).\n",
    "* <code>wind_speed</code>: The rate of horizontal travel of air past a fixed point (m/s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_feather(\n",
    "    os.path.join(data_dir, 'raw/weather.feather')\n",
    ")\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get general information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DataFrame is composed of chronological weather measurement (continuous variables). So, we recommend plotting the distribution of each column to have visual sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (24, 12)})\n",
    "sns.set(font_scale=1.5)\n",
    "f, axes = plt.subplots(2, 3)\n",
    "axes = axes.flatten()\n",
    "\n",
    "color = \"dodgerblue\"\n",
    "\n",
    "# \"airTemperature\" histogram\n",
    "ax1 = axes[0]\n",
    "g1 = sns.distplot(weather[\"air_temperature\"].dropna(), ax=ax1, color=color)\n",
    "ax1.set_title('Air temperature (ÂºC)')\n",
    "ax1.set(xlabel=\"\")\n",
    "\n",
    "# \"dewTemperature\" histogram\n",
    "ax2 = axes[1]\n",
    "g2 = sns.distplot(weather[\"dew_temperature\"].dropna(), ax=ax2, color=color)\n",
    "ax2.set_title('Dew temperature (ÂºC)')\n",
    "ax2.set(xlabel=\"\")\n",
    "\n",
    "# \"precipDepth1HR\" histogram\n",
    "ax3 = axes[2]\n",
    "g3 = sns.distplot(weather[\"precip_depth_1_hr\"].dropna(),\n",
    "                  ax=ax3, color=color, kde_kws={'bw': 0.1})\n",
    "ax3.set_title('Precipitation Depth in 1 hour (mm)')\n",
    "ax3.set(xlabel=\"\")\n",
    "\n",
    "# \"seaLvlPressure\" histogram\n",
    "ax5 = axes[3]\n",
    "g5 = sns.distplot(weather[\"sea_level_pressure\"].dropna(), ax=ax5, color=color)\n",
    "ax5.set_title('Pressure (hPa)')\n",
    "ax5.set(xlabel=\"\")\n",
    "\n",
    "# \"windSpeed\" histogram\n",
    "ax6 = axes[4]\n",
    "g6 = sns.distplot(weather[\"wind_speed\"].dropna(), ax=ax6, color=color)\n",
    "ax6.set_title('Wind speed (m/s)')\n",
    "ax6.set(xlabel=\"\")\n",
    "\n",
    "# \"windDirection\" polar histogram\n",
    "degrees = weather[\"wind_direction\"]\n",
    "radians = np.deg2rad(weather[\"wind_direction\"])\n",
    "bin_size = 20\n",
    "a, b = np.histogram(degrees, bins=np.arange(0, 360+bin_size, bin_size))\n",
    "centers = np.deg2rad(np.ediff1d(b)//2 + b[:-1])\n",
    "ax7 = f.add_subplot(248, projection='polar')\n",
    "ax7.set_theta_zero_location(\"N\")\n",
    "ax7.set_theta_direction(\"clockwise\")\n",
    "g7 = plt.bar(centers, a, width=np.deg2rad(bin_size),\n",
    "             bottom=0.0, color=color, alpha=0.6, edgecolor='k')\n",
    "ax7.set_title('Wind direction', pad=5, loc=\"left\")\n",
    "\n",
    "# Remove empty axes\n",
    "f.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see abnormal values of air temperature superior to 50 degrees Â°C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we notice that the high air temperature are in site_id 14\n",
    "sites_with_temperature_above_50 = (\n",
    "    weather\n",
    "    [weather.air_temperature > 50]\n",
    "    .site_id\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "print(f'Sites with temperature above 50Â°C: {sites_with_temperature_above_50}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we read the documentation from API data provider, we understood that the air temperature for the site_id 14 is provide in Fahrenheit (Â°F)\n",
    "\n",
    "To convert Fahrenheit (Â°F) to Celsius (Â°C)  (Fahrenheit - 32) * 5/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_to_c(fahrenheit):\n",
    "    return (fahrenheit - 32) * 5/9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use lambda notation and apply fuction from pandas to correct the air temperature values of the side_id number 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_temperature_C = (\n",
    "    weather\n",
    "    [weather.site_id == 14]\n",
    "    .air_temperature\n",
    "    .apply(lambda x: f_to_c(x))\n",
    ")\n",
    "weather.loc[weather.site_id == 14, 'air_temperature'] = air_temperature_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generally, there are missing lines in the chronological data due to an irregular timestamp. Let's check it ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_timestamp_contunity(df):\n",
    "    diff_timestamp = (df.timestamp - df.timestamp.shift(1)) / timedelta(hours=1)\n",
    "    flag_continuty_timestamp = 'OK' if diff_timestamp.var() == 0 else 'KO'\n",
    "    return flag_continuty_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check timestamp contunity')\n",
    "for site in weather.site_id.unique():\n",
    "    df_site = weather[weather.site_id == site].sort_values('timestamp')\n",
    "    flag_continuty_timestamp = check_timestamp_contunity(df_site)\n",
    "    print(f'site_id {site}: {flag_continuty_timestamp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_dfs = []\n",
    "for site in weather.site_id.unique():\n",
    "    new_idx = pd.date_range(start='2016-1-1', end='2017-12-31-23', freq='H')\n",
    "    site_df = weather[weather.site_id == site].set_index('timestamp').reindex(new_idx)\n",
    "    site_df.site_id = site\n",
    "\n",
    "    for col in [c for c in site_df.columns if c != 'site_id']:\n",
    "        site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n",
    "        site_df[col] = site_df[col].fillna(weather[col].median())\n",
    "    site_dfs.append(site_df)\n",
    "\n",
    "df = pd.concat(site_dfs)\n",
    "weather = df.reset_index().rename(columns={'index':'timestamp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result of cleaning weather in the folder clean to use for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_feather(\n",
    "    os.path.join(data_dir, 'clean/weather.feather'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It's your turn !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Meters\n",
    "\n",
    "The third file contains contains the actual energy consumption of the buildings in 2016 and 2017 (every hour).   \n",
    "There are four different energy types (electricity, chilledwater, steam, hotwater).  \n",
    "In each line the `meter` variable indicates the type of energy and the `meter_reading` the consumption.\n",
    "\n",
    "\n",
    "\n",
    "#### Meter readings\n",
    "\n",
    "- `timestamp`: date and time in the format YYYY-MM-DD hh:mm:ss. 2016 and 2017 data.\n",
    "- `building_id`: unique identifier for the buildings.\n",
    "- `meter_reading`: meter reading in kilowatt hour (kWh) .\n",
    "- `meter`: meter type, `chilledwater`, `electricity`, `hotwater`, `steam`\n",
    "\n",
    "\n",
    "The `meter` variable is encoded as an integer as follow:\n",
    "```json\n",
    "{0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO \n",
    "\n",
    "1. Read the meters reading file and show the 5 first rows\n",
    "2. Show general information about meters dataframe\n",
    "3. Create a loop over name columns to check the existence of nan values\n",
    "4. Show duplicates rows if exist and drop it\n",
    "5. Check if there are any inconsistent data in the `meter` column\n",
    "6. Is there  an uncommon values in meter_reading ?\n",
    "7. Save the cleaned dataframe\n",
    "\n",
    "### Hints\n",
    "\n",
    "- **Hint 1**:\n",
    "\n",
    "```python\n",
    "meters = pd.read_feather(\n",
    "    os.path.join(data_dir, 'raw/meters.feather')\n",
    ")\n",
    "meters.head()\n",
    "```\n",
    "\n",
    "\n",
    "- **Hint 2**:\n",
    "\n",
    "```python\n",
    "meters.info()\n",
    "````\n",
    "\n",
    "- **Hint 3**:\n",
    "\n",
    "```python\n",
    "print('The perc. of empty values')\n",
    "for column_name in meters.columns:\n",
    "    percentage_na = meters[column_name].isna().mean() * 100\n",
    "    print(f'{column_name} : {np.round(percentage_na)} %')\n",
    "```\n",
    "\n",
    "- **Hint 4**:\n",
    "\n",
    "```python\n",
    "mask_duplicated = meters.duplicated(keep=False)\n",
    "meters[mask_duplicated]\n",
    "meters = meters.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "numbers_rows = meters.shape[0]\n",
    "numbers_columns = meters.shape[1]\n",
    "print(f'meters without duplicates rows : {numbers_rows} | columns : {numbers_columns}')\n",
    "```\n",
    "\n",
    "- **Hint 5**:\n",
    "\n",
    "```python\n",
    "# show all unique values of `meter`\n",
    "print(meters.meter.unique())\n",
    "# drop the lines with `meter == -1`\n",
    "meters = meters[meters.meter != -1].reset_index(drop=True)\n",
    "```\n",
    "\n",
    "- **Hint 6**:\n",
    "\n",
    "```python\n",
    "meters.meter_reading.describe()\n",
    "meters = meters[meters.meter_reading > 0].reset_index(drop=True)\n",
    "```\n",
    "\n",
    "- **Hint 7**:\n",
    "\n",
    "```python\n",
    "meters.to_feather(\n",
    "    os.path.join(data_dir, 'clean/meters.feather')\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the `meters` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get general information about the dataset ? (# of rows, # of columns, NaN values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a duplicates rows ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many categories of unique meters exist in the dataset ?\n",
    "\n",
    "Is there an incoherent data ? If yes, drop the concerning rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about meter_reading?\n",
    "\n",
    "Compute statistics to check continuous variable.   \n",
    "Is there an incoherent data ? If yes, drop the concerning rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result of cleaning meters in the folder clean to use for the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Edit variable types / formats\n",
    "- Identify duplicates\n",
    "- Delete columns with many missing values\n",
    "- Use common sense and keep only relevant variables\n",
    "- Observe the distribution of values of a variable\n",
    "- Visual representations are useful to understand how a variable works\n",
    "\n",
    "### Pitfalls to avoid\n",
    "- Automatically delete a duplicate: understand why the duplicate appeared\n",
    "- Automatically delete all rows with missing values and lose information. Approximating some values allows you to keep information to meet an objective.\n",
    "- Automatically delete outliers: understand where they come from, are they errors or do they only represent extreme cases?\n",
    "- Retain variables that could be harmful to the ethics of a project (skin color, address...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go Further :\n",
    "- [The Ultimate Guide to Data Cleaning](https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4)\n",
    "- [Learn Data Cleaning Tutorials | Kaggle](https://www.kaggle.com/learn/data-cleaning)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3207b46d55f06357cbc786d58801af30adee4e85f2fbb9c5029957ffa03d32a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
