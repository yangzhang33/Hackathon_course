{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "024e8181",
   "metadata": {},
   "source": [
    "## 3. Feature_engineering_and_algorithm_setup\n",
    "\n",
    "Authors : Haddam Yacine, Ka Alioune, Renaud Adrien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b626b8d5",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"../src/figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae0c69",
   "metadata": {},
   "source": [
    "In this lab, we briefly recall some machine learning basics, and we are interested in a problem of building a regression model using machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668da36",
   "metadata": {},
   "source": [
    "## What is a machine learning model:\n",
    "\n",
    "The Building a machine learning model can be summed up in finding a link function $f$\n",
    " ($Y=f(X) + \\epsilon$) which is most often the\n",
    "result of error minimization : <p style=\"text-align: center;\">$\\sum_i E(Y_i,f(X_i))$</p> where\n",
    "$(X_i,Y_i)$ is a list of pairs (features, target).\n",
    "\n",
    "**Objective:** \n",
    "- Train the model from a dataset and assess its ability to generalize on unseen data\n",
    "- Understand the explanatory factors of our target\n",
    "    \n",
    "**Method:**\n",
    "- Separate the target variable from the features\n",
    "- separate the data into three samples (train / validation / test)\n",
    "- train the model (on the train set) and evaluate its performance (on the test set)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0bc5b",
   "metadata": {},
   "source": [
    "## Instructions for this tutorial:\n",
    "- Load the dataset from workspace.\n",
    "- Train a regression tree to predict the `meter_reading`, plot the importance of each of the features of the database. Evaluate with metrics on train and test.\n",
    "- Train multiple regression trees by varying the max_depth parameter. Evaluate performance with metrics, on train and test, and plot the curve of these metrics as a function of max_depth.\n",
    "- Train multiple random forests by varying some parameters. Evaluate performance with metrics, on train and test, and plot the curve of these metrics as a function of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bf93c",
   "metadata": {},
   "source": [
    "We will use the [sklearn package](https://fr.wikipedia.org/wiki/Scikit-learn) for models and various metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2ad38",
   "metadata": {},
   "source": [
    "### Data Path\n",
    "\n",
    "`data_dir` is the path to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jovyan/personal_workspace/bootcamp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44655192",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe60c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761430a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "sys.path.append('../src/notebooks')\n",
    "from utils.get_data import load_data\n",
    "from utils.utils_model import plot_importance, plot_max_depth_influence, plot_n_estimators_influence\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d507588",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_feather(os.path.join(data_dir, \"merged/data.feather\"))\n",
    "\n",
    "# remove building 1099 that has abnormal behaviour\n",
    "data = data[data.building_id != 1099]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64518686",
   "metadata": {},
   "source": [
    "## Features of the Dataset\n",
    "\n",
    "- Building metadata\n",
    "   * `building_id`: unique identifier of the building.\n",
    "   * `site_id`: unique identifier of the site.\n",
    "   * `primaryspaceusage`: Primary space usage of all buildings is mapped using the [energystar scheme building description types](https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/identify-your-property-type). \n",
    "   * `square_feet`: Floor area of building in square meters (m2).\n",
    "   *  `lat`: Latitude of building location to site level.\n",
    "   *  `lng`: Longitude of building location to site level.\n",
    "\n",
    "\n",
    "- Weather data\n",
    "   * <code>timestamp</code>: date and time in the format YYYY-MM-DD hh:mm:ss. Local timezone.\n",
    "   * <code>site_id</code>: unique identifier of the site.\n",
    "   * <code>air_temperature</code>: The temperature of the air in degrees Celsius (ºC).\n",
    "   * <code>cloud_coverage</code>: Portion of the sky covered in clouds, in [oktas](https://en.wikipedia.org/wiki/Okta).\n",
    "   * <code>dew_temperature</code>: The dew point (the temperature to which a given parcel of air must be cooled at constant pressure and water vapor content in order for saturation to occur) in degrees Celsius (ºC).\n",
    "   * <code>precip_depth_1_hr</code>: The depth of liquid precipitation that is measured over a one hour accumulation period (mm).\n",
    "   * <code>sea_lvl_pressure</code>: The air pressure relative to Mean Sea Level (MSL) (mbar or hPa).\n",
    "   * <code>wind_direction</code>: The angle, measured in a clockwise direction, between true north and the direction from which the wind is blowing (degrees).\n",
    "   * <code>wind_speed</code>: The rate of horizontal travel of air past a fixed point (m/s).\n",
    "\n",
    "\n",
    "- Meter reading data\n",
    "    *   `timestamp`: date and time in the format YYYY-MM-DD hh:mm:ss. 2016 and 2017 data.\n",
    "    *   `building_id`: unique identifier of the building.\n",
    "    *   `meter_reading`: meter reading in kilowatt hour (kWh) .\n",
    "    *   `meter`: meter type, `chilledwater`, `electricity`, `hotwater` or `steam` .\n",
    "\n",
    "```json\n",
    "{0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760e11b",
   "metadata": {},
   "source": [
    "It is always a good practice to explicitly state the features that we would like to use before training machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a26a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_use = [\n",
    "    'site_id', 'building_id',\n",
    "    'timestamp',\n",
    "    'lat', 'lng',\n",
    "    'primary_use',\n",
    "    'square_feet',\n",
    "    'air_temperature', 'dew_temperature', 'precip_depth_1_hr', 'wind_speed', 'sea_level_pressure', 'wind_direction',\n",
    "    'meter_name',\n",
    "    'meter_reading'\n",
    "]\n",
    "data = data[columns_to_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb9083",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a67501",
   "metadata": {},
   "source": [
    "Feature engineering is the process by which knowledge of data is used to construct explanatory variables, features, that can be used to train a predictive model. Engineering and selecting the correct features for a model will not only significantly improve its predictive power, but will also offer the flexibility to use less complex models that are faster to run and more easily understood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bd7ff",
   "metadata": {},
   "source": [
    "Based on [EDA of meter readings]():\n",
    "\n",
    "*   _Healthcare_,  and _Education_ usages shows the highest meter reading values while _Parking_ shows lower.\n",
    "*   _Hotwater_ meter shows the highest meter reading values.\n",
    "*   Monthly behaviour (meter-reading median) shows higher readings in warm season.\n",
    "*   Hourly behaviour (meter-reading median) shows higher values from 6 to 19 hs.\n",
    "*   Weekday behaviour: lowers during weekends.\n",
    "* There is also a significant difference between sites\n",
    "\n",
    "Based on these conclusions, we can create new features to in order to have an accurate model or to avoid complex one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4515dd",
   "metadata": {},
   "source": [
    "### Feature Engineering from timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18d3cc",
   "metadata": {},
   "source": [
    "The timestamp in itself is not a useful feature. But we can extract from it some powerful features like the hour of the day or the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f81cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hour\n",
    "data[\"hour\"] = data[\"timestamp\"].dt.hour.astype(\"int8\")\n",
    "# days of the week (mon=0 and sun=6)\n",
    "data[\"weekday\"] = data[\"timestamp\"].dt.dayofweek.astype(\"int8\")\n",
    "# month\n",
    "data[\"month\"] = data[\"timestamp\"].dt.month.astype(\"int8\")\n",
    "# year\n",
    "data[\"year\"] = data[\"timestamp\"].dt.year.astype(\"int16\")\n",
    "# days (1 to 31)\n",
    "data[\"day\"] = data[\"timestamp\"].dt.day.astype(\"int8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00e741e",
   "metadata": {},
   "source": [
    "Do you think that there is some other features that could be interesting?\n",
    "\n",
    "```python\n",
    "# business hours\n",
    "data['is_wider_busness_hours'] = np.where((data[\"hour\"] >= 7) & (data[\"hour\"] <=19 ), 1, 0)\n",
    "\n",
    "# Weekend\n",
    "data['is_weekend'] = np.where((data[\"weekday\"] >= 0) & (data[\"weekday\"] <= 4), 0, 1)\n",
    "\n",
    "# Season of year\n",
    "data['season'] = (np.where(data[\"month\"].isin([12, 1, 2]), 0,\n",
    "                   np.where(data[\"month\"].isin([3, 4, 5]), 2,         \n",
    "                   np.where(data[\"month\"].isin([6, 7, 8]), 3,          \n",
    "                   np.where(data[\"month\"].isin([9, 10, 11]), 1, 0)))))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf847d0",
   "metadata": {},
   "source": [
    "### Feature Engineering from geographic position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c04ea",
   "metadata": {},
   "source": [
    "We have seen a difference between US and European sites. Lets create a feature for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"zone_geo\"] = \"US\"\n",
    "data.loc[data.lng > -4, 'zone_geo'] = \"EUROPE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6c6d0",
   "metadata": {},
   "source": [
    "### Feature Engineering  from transformation \n",
    "\n",
    "Feature transformations can include aggregating, combining transforming attributes to create new features. Useful and relevant features will depend on the problem at hand but averages, sums, log or ratios can better expose trends to a model.\n",
    "\n",
    "We can also transform a numerical feature into a categorical feature by cutting it into classes. This can be interesting to avoid the impact of outliers or to reduce the variance of the output variable.\n",
    "\n",
    "##### Example: \n",
    "\n",
    "```python\n",
    "# log transformation\n",
    "data['square_feet_log'] = data['square_feet'].apply(np.log)\n",
    "\n",
    "# polynomial transformation\n",
    "data['air_temperature_squared'] = data['air_temperature']**2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5b5ec",
   "metadata": {},
   "source": [
    "## Encoding your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74a64d",
   "metadata": {},
   "source": [
    "Some algorithms can work with categorical data directly. This means that categorical data must be converted to a numerical form. \n",
    "To Convert Categorical Data to Numerical Data, this involves two steps :\n",
    "\n",
    "- Integer  (ordinal or cardinal)\n",
    "- One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c9f25",
   "metadata": {},
   "source": [
    "<img src=\"../src/figures/encoding.png\" width=900 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d3c08b",
   "metadata": {},
   "source": [
    "Use [pd.get_dummies()](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) for OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to encode using One-Hot-Encoding\n",
    "columns_to_encode = [\"meter_name\", \"primary_use\", \"zone_geo\", \"site_id\"]\n",
    "\n",
    "# encode those columns\n",
    "encoded_data = pd.get_dummies(data[columns_to_encode], columns=columns_to_encode)\n",
    "\n",
    "# add encoded columns to the data\n",
    "data = pd.concat([data, encoded_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff54c61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b773e0",
   "metadata": {},
   "source": [
    "## Train / Validation / Test Split "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c131fad3",
   "metadata": {},
   "source": [
    "The **train-validation-test** split is a technique for training and evaluating the performance of a machine learning algorithm.\n",
    "\n",
    "The **procedure** involves taking a dataset and dividing it into three subsets:\n",
    "- The **train** subset is used to fit the model and is referred to as the training dataset. You should not evaluate the performance of the model on this train set,\n",
    "- The **validation** subset is used to tune hyperparameters of an algorithm. For example the *max_depth* for a regression tree,\n",
    "- The **test** subset is not used to train the model; it is only used at the end to evaluate the performances of the model. For a purist, once you 'opened' the test set, you should not modify the model anymore.\n",
    "\n",
    "\n",
    "**Strategy**\n",
    "   - Define a test set, and separate the remaining between train and validation.\n",
    "   - Generally they represent respectively 70% | 15% | 15% of the initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757b3cf",
   "metadata": {},
   "source": [
    "<img src=\"../src/figures/train_test_1.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2443b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[\"timestamp\"].between(\"2016-01-01 00:00:00\", \"2016-12-31 23:00:00\")].reset_index(drop=True)\n",
    "val = data[data[\"timestamp\"].between(\"2017-01-01 00:00:00\", \"2017-09-01 00:00:00\")].reset_index(drop=True)\n",
    "test = data[data[\"timestamp\"] > \"2017-09-01 00:00:00\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of examples in the train set: {train.shape[0]}\")\n",
    "print(f\"Number of examples in the val set:   {val.shape[0]}\")\n",
    "print(f\"Number of examples in the test set:  {test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88307",
   "metadata": {},
   "source": [
    "</h3><font color='red'>  ! Be sure that all buildings in the test and validation sets are also in train set !</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb850ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique buildings in tain set\n",
    "train_building = train.building_id.unique().tolist()\n",
    "\n",
    "# Keep only buildings that are in train set\n",
    "train = train[train.building_id.isin(train_building)].reset_index(drop=True)\n",
    "test = test[test.building_id.isin(train_building)].reset_index(drop=True)\n",
    "val = val[val.building_id.isin(train_building)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6222da",
   "metadata": {},
   "source": [
    "## Splitting your data (Features / Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7879f599",
   "metadata": {},
   "source": [
    "It is a rather practical approach because generally the algorithms of machine learning ask for the features on the one hand and the target on the other hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026c046",
   "metadata": {},
   "source": [
    "<img src=\"../src/figures/split_columns.png\" width=700 height=500 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0274397",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # 'building_id',\n",
    "    'lat',\n",
    "    'lng',\n",
    "    'square_feet',\n",
    "    'air_temperature',\n",
    "    'dew_temperature',\n",
    "    'precip_depth_1_hr',\n",
    "    'wind_speed',\n",
    "    'sea_level_pressure',\n",
    "    'wind_direction',\n",
    "    'hour',\n",
    "    'weekday',\n",
    "    'month',\n",
    "    'meter_name_chilledwater',\n",
    "    'meter_name_electricity',\n",
    "    'meter_name_hotwater',\n",
    "    'meter_name_steam',\n",
    "    'primary_use_Education',\n",
    "    'primary_use_Entertainment/public assembly',\n",
    "    'primary_use_Healthcare',\n",
    "    'primary_use_Industry',\n",
    "    'primary_use_Lodging/residential',\n",
    "    'primary_use_Office',\n",
    "    'primary_use_Other',\n",
    "    'primary_use_Parking',\n",
    "    'primary_use_Public services',\n",
    "    'primary_use_Services',\n",
    "    'zone_geo_EUROPE',\n",
    "    'zone_geo_US',\n",
    "    'site_id_0',\n",
    "    'site_id_1',\n",
    "    'site_id_2',\n",
    "    'site_id_3',\n",
    "    'site_id_4',\n",
    "    'site_id_5',\n",
    "    'site_id_6',\n",
    "    'site_id_7',\n",
    "    'site_id_9',\n",
    "    'site_id_11',\n",
    "    'site_id_12',\n",
    "    'site_id_13',\n",
    "    'site_id_15',\n",
    "]\n",
    "\n",
    "target = \"meter_reading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"::: Number of features {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6652f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets for later use\n",
    "\n",
    "SAVED_DATA_MODEL = True\n",
    "\n",
    "PATH_MODEL_DATA = os.path.join(data_dir, \"model\")\n",
    "if not(os.path.exists(PATH_MODEL_DATA)):\n",
    "    os.mkdir(PATH_MODEL_DATA)\n",
    "\n",
    "if SAVED_DATA_MODEL:\n",
    "    train[features + [target]].to_feather(os.path.join(PATH_MODEL_DATA, 'train.feather'))\n",
    "    val[features + [target]].to_feather(os.path.join(PATH_MODEL_DATA, 'val.feather'))\n",
    "    test[features + [target]].to_feather(os.path.join(PATH_MODEL_DATA, 'test.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e95601",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for a Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a850d1",
   "metadata": {},
   "source": [
    " In this lab, as we are interested in a regression problem, we will first see some classic regression metrics.\n",
    "\n",
    "Suppose we evaluate these metrics for a set $(y_i, \\hat{y}_i)_{i=1,...n_{\\text{test}}}$, where $y_i$ is the true value and   $\\hat{y_i}$ is the prediction.\n",
    "\n",
    "\n",
    "- **mean absolute error**:   \n",
    "$\\text{MAE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1,...n_{\\text{test}}} |y_i - \\hat{y_i}|$ .\n",
    "\n",
    "- **mean squared error**:   \n",
    "$\\text{MSE} = \\frac{1}{n_{\\text{test}}} \\sum_{i=1,...n_{\\text{test}}} (y_i - \\hat{y_i})^2$. The most used.\n",
    "\n",
    "- **max error**: \n",
    "$\\text{MAX_Error} =  \\max_{i=1,...n_{\\text{test}}} (y_i - \\hat{y_i})$. Calculates the maximum residual error. It is very sensitive to outliers\n",
    "\n",
    "MSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5.\n",
    "\n",
    "From an interpretation standpoint, MAE is clearly the winner. MSE does not describe average error alone and has other implications that are more difficult to tease out and understand.\n",
    "\n",
    "<img src=\"../src/figures/metric.PNG\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fd703",
   "metadata": {},
   "source": [
    "## Regression trees\n",
    "\n",
    "In this lab, we are interested in regression trees and random forests, for our regression problem. They are among the simplest models in machine learning, but remain important because they are one of the few models that remain explainable: it is the opposite of the black box model.\n",
    "\n",
    "A regression tree is a set of ordered decision rules, as in the figure below. There are many types of decision trees, but in machine learning they often take the form of binary trees. By going through the tree, we go through a succession of questions on the variables of the database, until ending up on a sheet, which allows us to know which output to assign to a given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb95df16",
   "metadata": {},
   "source": [
    "<img src=\"../src/figures/arbre_1.png\" width=700 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebbee1b",
   "metadata": {},
   "source": [
    "At this stage, we can ask the following question: a given tree is easy to read, but on what bases, on what rules was it built? A classic tree-building algorithm is the CART (Classification and Regression Trees) algorithm.\n",
    "\n",
    "To formalize a little, we can say that in this algorithm, we suppose that the tree divides the input space into M regions $ R1, ..., RM $. The value assigned in output for an input x can be represented in the form of a decision function f such as:\n",
    "\n",
    "<p style = \"text-align: center;\"> $f (x) = \\sum_{m = 1, ..., M} c_m \\times \\mathbb {I} \\left (x \\in R_m \\right) $ </p>\n",
    " \n",
    "Where the $ c_m $ are constants to be determined during training.\n",
    "\n",
    "$\\textbf {When do we stop splitting our regions?}$ The depth of the tree is generally considered to be a hyperparameter to be optimized. The classical algorithms therefore split in a binary way until the size of the tree reaches this parameter (which will be called in the following\n",
    "$ max \\_depth $), then then do some pruning. Pruning consists of removing certain nodes from the tree, based on a cost-complexity function. [see here](https://towardsdatascience.com/decision-tree-classifier-and-cost-computation-pruning-using-python-b93a0985ea77).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f6e16",
   "metadata": {},
   "source": [
    "**PROS**\n",
    "\n",
    "- direct translation of the tree into a rule base\n",
    "- undifferentiated treatment of different types of predictor variables\n",
    "- robust against outliers, solutions for missing data\n",
    "- speed and ability to handle very large databases\n",
    "\n",
    "**CONS**\n",
    "\n",
    "- stability problem\n",
    "- poorer performance in general compared to other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8acc6e",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9284c4a0",
   "metadata": {},
   "source": [
    "We fit a decision tree on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ccb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "\n",
    "model_tree = tree.DecisionTreeRegressor(max_depth=3, random_state=0)\n",
    "model_tree = model_tree.fit(train[features], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05b026",
   "metadata": {},
   "source": [
    "We can visualize the fitted decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390dd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25, 20))\n",
    "_ = tree.plot_tree(model_tree,\n",
    "                   feature_names=train[features].columns,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeb4a69",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6b66a",
   "metadata": {},
   "source": [
    "Evaluate model performance on the validation set.  \n",
    "For this we compute the MAE, MSE and maximum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dbcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    max_error,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_log_error,\n",
    ")\n",
    "\n",
    "\n",
    "def print_errors(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(f'MAE : {mean_absolute_error(y, y_pred):.0f}')\n",
    "    print(f'MSE : {mean_squared_error(y, y_pred):.0f}')\n",
    "    print(f'MAX : {max_error(y, y_pred):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192620ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_errors(model_tree, val[features], val[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa84659c",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b893a",
   "metadata": {},
   "source": [
    "Feature importance refers to a class of techniques for assigning scores to input features to a predictive model that indicates the relative importance of each feature when making a prediction.\n",
    "\n",
    "The scores are useful and can be used in a range of situations in a predictive modeling problem, such as:\n",
    "\n",
    "- Better understanding a model.\n",
    "- Reducing the number of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b09872",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model_tree, train[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f4d2d3",
   "metadata": {},
   "source": [
    "## [Bias / Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d98d3",
   "metadata": {},
   "source": [
    "<img src=\"../src/figures/overfiting.PNG\" width=600 height=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f5ad4",
   "metadata": {},
   "source": [
    "$\\textbf{UNDERFITTING :}$ The model is too simple to capture the relationships between the data\n",
    "\n",
    "*Solutions*:\n",
    "- Introduce more features\n",
    "- Increase model complexity\n",
    "\n",
    "\n",
    "$\\textbf{OVERFITTING :}$ The model is too complex and sticks too closely to the training data\n",
    "\n",
    "*Solutions*:\n",
    "\n",
    "- Decrease model complexity\n",
    "- Include more data\n",
    "- Use regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b38465",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth_ls = [2, 4, 8, 16, 32, 48]\n",
    "plot_max_depth_influence(\n",
    "    max_depth_ls,\n",
    "    train[features], train[target],\n",
    "    val[features], val[target],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f79213b",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "The main problem with decision trees is their large variance: a tiny error at the top of the tree is propagated all the way down the tree and it gets worse quickly. To stabilize the tree's predictions, we prefer to generate a set of trees, a forest and this algorithm is called *Random Forest*.\n",
    "\n",
    "\n",
    "To create a random forest with B trees, we proceed as follows:\n",
    "\n",
    "- For i ranging from 1 to B:\n",
    "  - We draw randomly with replacement a sub-sample of the data size $ n <n _ {\\ text {train}} $\n",
    "  - We randomly draw a subsample of features of size m with in general $ m \\leq \\sqrt {p} $\n",
    "  - On this new dataset composed of n examples and m features, we train a decision tree of fixed max depth\n",
    "- We thus obtain $ B $ decision trees. If we denote by $ f1, ..., fB $ the prediction functions of each tree, then in regression, the decision function of the forest $ f_ {RF} $ will be:\n",
    "<p style = \"text-align: center;\"> $ f_{RF} (x) = \\frac{1}{B} \\sum_{i = 1, ..., B} f_i (x)$ </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d43dbe",
   "metadata": {},
   "source": [
    "<img src=\"../src/figures/RF.png\" width=900 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b951722a",
   "metadata": {},
   "source": [
    "### The mains parameters\n",
    "\n",
    "- *n_estimators* : number of trees in the foreset\n",
    "\n",
    "- *max_features* : max number of features considered for splitting a node\n",
    "\n",
    "- *max_depth* : max number of levels in each decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb91c90",
   "metadata": {},
   "source": [
    "### Fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef20069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=10,\n",
    "    max_depth=16,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_rf.fit(train[features], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0eaeb",
   "metadata": {},
   "source": [
    "### Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e7de4",
   "metadata": {},
   "source": [
    "Evaluate model performance on the validation set.  \n",
    "For this we compute the MAE, MSE and maximum error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2954e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_errors(model_rf, val[features], val[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4f10c",
   "metadata": {},
   "source": [
    "### Plot importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(model_rf, train[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc563f",
   "metadata": {},
   "source": [
    "## To Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ba653",
   "metadata": {},
   "source": [
    "<font color='blue'> <h2> Feature Engeneering </h2> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71f48fc",
   "metadata": {},
   "source": [
    "1. Based of the feature importance of the Regression tree, drop some insignificant features on *train* and fit a new model. Is your model better now ?\n",
    "\n",
    "2. Using your background, create a new feature who seems interesting (do it on train, val and test). What is the impact of this feature on your model ?\n",
    "\n",
    "\n",
    "**Hint 1**\n",
    "\n",
    "```python\n",
    "selected_features = [...]\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=10,\n",
    "    max_depth=16,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_rf.fit(train[selected_features], train[target])\n",
    "print_errors(model_rf, val[selected_features], val[target])\n",
    "```\n",
    "\n",
    "**Hint 2**\n",
    "\n",
    "```python\n",
    "# Weekend\n",
    "train['is_weekend'] = np.where((train[\"weekday\"] >= 0) & (train[\"weekday\"] <= 4), 0, 1)\n",
    "val['is_weekend'] = np.where((val[\"weekday\"] >= 0) & (val[\"weekday\"] <= 4), 0, 1)\n",
    "\n",
    "model_rf = RandomForestRegressor(\n",
    "    n_estimators=10,\n",
    "    max_depth=16,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "model_rf.fit(train[features + [\"is_weekend\"]], train[target])\n",
    "print_errors(model_rf, val[features + [\"is_weekend\"]], val[target])\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219fea0",
   "metadata": {},
   "source": [
    "<font color='blue'> <h2> Bias / Variance Trade-off with hyperparameters </h2> </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8a609",
   "metadata": {},
   "source": [
    "1. Using the validation sample, vary the parameter `n_estimators` of the random forest from 1 to 20 in steps of ~5.\n",
    "\n",
    "2. Analyze the results and give the optimal number of tree.\n",
    "\n",
    "**Hint**: use the `plot_n_estimators_influence()` from `utils_model.py` which takes an integer list as argument:\n",
    "```python\n",
    "n_estimators = [1, 2, 5, 10, 15, 30]\n",
    "plot_n_estimators_influence(n_estimators, X_train, y_train, X_val, y_val)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a15384",
   "metadata": {},
   "source": [
    "<font color='blue'> <h2> Analyze deeper your model performances </h2> </font> \n",
    "\n",
    "1. Add the predictions of your model to the `val` dataframe\n",
    "2. For each lines compute the absolute error\n",
    "3. Compute also the absolute percentage error\n",
    "4. What can you say about your model predictions? Are they as good for every building, day...\n",
    "\n",
    "\n",
    "**Hint 1 - 2 - 3**\n",
    "\n",
    "```python\n",
    "val[\"meter_reading_prediction\"] = model_rf.predict(val[features])\n",
    "val[\"MAE\"] = (val[\"meter_reading\"] - val[\"meter_reading_prediction\"]).abs()\n",
    "val[\"MAPE\"] = (val[\"meter_reading\"] - val[\"meter_reading_prediction\"]).abs() / val[\"meter_reading\"] * 100\n",
    "```\n",
    "\n",
    "\n",
    "**Hint 4**\n",
    "\n",
    "```python\n",
    "val[val.meter_reading > 0.1].groupby(\"building_id\").MAPE.median().sort_values()\n",
    "\n",
    "(\n",
    "    val[val.meter_reading > 0.1]\n",
    "    .set_index(\"timestamp\")\n",
    "    .resample(\"1D\")\n",
    "    .MAPE\n",
    "    .median()\n",
    "    .plot.line(figsize=(20, 10))\n",
    ")\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "val[val.meter_reading > 0.1].plot.scatter(\"meter_reading\", \"meter_reading_prediction\", alpha=0.1, figsize=(10, 10))\n",
    "plt.xlim(0, 10000)\n",
    "plt.ylim(0, 10000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320fe24d",
   "metadata": {},
   "source": [
    "## Go further!\n",
    "- Hastie T., Tibshirani R., Friedman J., « [The elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) - Data Mining, Inference and Prediction », pringer, 2009."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3207b46d55f06357cbc786d58801af30adee4e85f2fbb9c5029957ffa03d32a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
