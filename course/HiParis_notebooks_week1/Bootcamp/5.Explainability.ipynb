{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d36a27e7",
   "metadata": {},
   "source": [
    "## 5.Explainability\n",
    "\n",
    "Authors : Haddam Yacine, Ka Alioune, Renaud Adrien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b7d3c",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"../src/figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99cd6b",
   "metadata": {},
   "source": [
    "Machine learning (ML) models are increasingly complex. Indeed, a sophisticated model (Random Forest, Boosting or deep learning) generally leads to more precise predictions than a simple model (linear regression or decision tree). There is thus a compromise between the performance of a model and its interpretability.\n",
    "\n",
    "Interpretability is defined as the ability for a human to understand the reasons for a model’s decision. This criterion has become preponderant for many reasons:\n",
    "\n",
    "- **Scientific**: It is about understanding, having confidence and having proof of the consistency and consistency of the model.\n",
    "\n",
    "\n",
    "- **Ethic**: It is unacceptable to entrust the fate of people or the economy to algorithms without being able to justify the decision-making process taken by these algorithms.\n",
    "\n",
    "\n",
    "- **Legislative**: [Article 22](https://www.cnil.fr/fr/profilage-et-decision-entierement-automatisee) of the RGPD (General Data Protection Regulation) provides that a person must not be the subject of a decision based exclusively on automated processing and emanating solely from the decision of a machine .\n",
    "\n",
    "In this lab , we present two methods of interpreting machine learning models: the **LIME** and **SHAP** algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b27aa",
   "metadata": {},
   "source": [
    "## Interpretability methods\n",
    "\n",
    "The different interpretability approaches can be defined according to the following typologies:\n",
    "\n",
    "- **Agnostic** versus **specific** interpretation methods: Agnostic methods can be used for any type of model. On the contrary, specific models can only be used to interpret a specific family of algorithms.\n",
    "\n",
    "\n",
    "- **Local** versus **global** methods: Local methods give an interpretation for a single or a small number of observations. On the contrary, global interpretation methods allow all observations to be explained at the same time, globally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1397d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7592b",
   "metadata": {},
   "source": [
    "### Install shap package\n",
    "\n",
    "We need to reinstall the package before doing anything else.\n",
    "\n",
    "And reload the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3010a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install --yes -c conda-forge --prefix {sys.prefix} shap\n",
    "!{sys.executable} -m pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae427e4b",
   "metadata": {},
   "source": [
    "### Data Path\n",
    "\n",
    "`data_dir` is the path to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jovyan/personal_workspace/bootcamp/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe1da0",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86952107",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather(os.path.join(data_dir, 'model/train.feather'))\n",
    "val = pd.read_feather(os.path.join(data_dir, 'model/val.feather'))\n",
    "test = pd.read_feather(os.path.join(data_dir, 'model/test.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4afe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # 'building_id',\n",
    "    'lat',\n",
    "    'lng',\n",
    "    'square_feet',\n",
    "    'air_temperature',\n",
    "    'dew_temperature',\n",
    "    'precip_depth_1_hr',\n",
    "    'wind_speed',\n",
    "    'sea_level_pressure',\n",
    "    'wind_direction',\n",
    "    'hour',\n",
    "    'weekday',\n",
    "    'month',\n",
    "    'meter_name_chilledwater',\n",
    "    'meter_name_electricity',\n",
    "    'meter_name_hotwater',\n",
    "    'meter_name_steam',\n",
    "    'primary_use_Education',\n",
    "    'primary_use_Entertainment/public assembly',\n",
    "    'primary_use_Healthcare',\n",
    "    'primary_use_Industry',\n",
    "    'primary_use_Lodging/residential',\n",
    "    'primary_use_Office',\n",
    "    'primary_use_Other',\n",
    "    'primary_use_Parking',\n",
    "    'primary_use_Public services',\n",
    "    'primary_use_Services',\n",
    "    'zone_geo_EUROPE',\n",
    "    'zone_geo_US',\n",
    "    'site_id_0',\n",
    "    'site_id_1',\n",
    "    'site_id_2',\n",
    "    'site_id_3',\n",
    "    'site_id_4',\n",
    "    'site_id_5',\n",
    "    'site_id_6',\n",
    "    'site_id_7',\n",
    "    'site_id_9',\n",
    "    'site_id_11',\n",
    "    'site_id_12',\n",
    "    'site_id_13',\n",
    "    'site_id_15',\n",
    "]\n",
    "\n",
    "target = \"meter_reading\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655c7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model optimization\n",
    "features = [\n",
    "    'square_feet',\n",
    "    \"site_id_13\",\n",
    "    'air_temperature', 'precip_depth_1_hr', 'wind_speed',\n",
    "    'hour', 'weekday', 'month',\n",
    "    'meter_name_chilledwater',\n",
    "    'meter_name_electricity',\n",
    "    'meter_name_hotwater',\n",
    "    'meter_name_steam',\n",
    "    'zone_geo_US',\n",
    "    'primary_use_Education'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9a140",
   "metadata": {},
   "source": [
    "## Fitting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor(\n",
    "    n_estimators=10,\n",
    "    max_depth=16,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "regressor.fit(train[features], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107d451",
   "metadata": {},
   "source": [
    "## LIME\n",
    "\n",
    "The LIME algorithm (Local Interpretable Model-agnostic Explanations) is a local model that seeks to explain the prediction of an individual by analyzing his neighborhood.\n",
    "\n",
    "LIME has the particularity of being a model:\n",
    "\n",
    "- *Interpretable*. It provides a qualitative understanding between the input variables and the response. The input-output relationships are easy to understand.\n",
    "\n",
    "- *Locally simple*. The model is globally complex, it is then necessary to look for locally simpler answers.\n",
    "\n",
    "- *Agnostic*. He is able to explain any machine learning model.\n",
    "\n",
    "<font color = \"red\"> The main drawback of the LIME method is linked to its local operation. And, LIME does not allow us to generalize the interpretability from the local model at a more global level. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd03a331",
   "metadata": {},
   "source": [
    "### LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f16151",
   "metadata": {},
   "source": [
    "Now, we go for the LIME. First we create our explainer with **LimeTabularExplainer**. This function need a train data set used to compute similarity of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c4613",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    np.array(train[features]),\n",
    "    feature_names=features,\n",
    "    verbose=True,\n",
    "    mode='regression'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed1d1a",
   "metadata": {},
   "source": [
    "### Explaining an instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66349af2",
   "metadata": {},
   "source": [
    "The LimeTabularExplainer has a method named **explain_instance()** which takes as input a local sample and method which predicts output. It generates explanation object and this Explanation object has information about feature contribution to this particular prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b754c",
   "metadata": {},
   "source": [
    "How can we explain this consumption according to our algorithm using LIME ?\n",
    "\n",
    "We randomly select a instance from site n° 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3eb4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_indice = 387608\n",
    "\n",
    "input_test = val[features].iloc[instance_indice]\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    input_test,\n",
    "    regressor.predict,\n",
    "    num_features=len(features),\n",
    "    num_samples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86f03a",
   "metadata": {},
   "source": [
    "We apply the lime methodology on this instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c95b0c",
   "metadata": {},
   "source": [
    "And we plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f171aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(explanation.as_list(), columns=[\"names\", \"coef\"])\n",
    "\n",
    "with plt.style.context(\"ggplot\"):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    plt.barh(range(len(results.coef)), results.coef, color=[\"green\" if coef < 0 else \"red\" for coef in results.coef])\n",
    "    plt.yticks(range(len(results.coef)), results.names);\n",
    "    plt.title(\"Local Explanation with LIME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba1476",
   "metadata": {},
   "source": [
    "**red** : features that lead to overconsumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e90aa2",
   "metadata": {},
   "source": [
    "### Warning !!!  \n",
    "\n",
    "The interpretation of LIME is strictly local and cannot be generalized on all of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c9cc7",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88808b",
   "metadata": {},
   "source": [
    "The goal of SHAP (SHapley Additive exPlanations) is to explain the prediction of an instance x by computing the contribution of each feature to the prediction. The SHAP explanation method computes Shapley values from coalitional game theory. The feature values of a data instance act as players in a coalition. Shapley values tell us how to fairly distribute the “payout” (= the prediction) among the features. A player can be an individual feature value, e.g. for tabular data. A player can also be a group of feature values.\n",
    "\n",
    "Shapley values can be combined into global explanations. If we run SHAP for every instance, we get a matrix of Shapley values. This matrix has one row per data instance and one column per feature. We can interpret the entire model (global) by analyzing the Shapley values in this matrix\n",
    "\n",
    "**Adventages**\n",
    "\n",
    "- fast implementation for tree-based models\n",
    "- global model interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca59d405",
   "metadata": {},
   "source": [
    "### Explainer\n",
    "\n",
    "Is this case, we call **TreeExplainer** - which is used for models that are based on a tree-like decision tree, random forest, gradient boosting. There are many other king of explainer for different machine learning model ([here](https://coderzcolumn.com/tutorials/machine-learning/shap-explain-machine-learning-model-predictions-using-game-theoretic-approach))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba389e20",
   "metadata": {},
   "source": [
    "### SHAP Summary Plot\n",
    "The summary plot combines feature importance with feature effects : \n",
    "\n",
    "- Variables are ranked according to feature importances in descending order . \n",
    "- The color represents the value of the feature from low to high. \n",
    "- Each point on the summary plot is a Shapley value for a feature and an instance. \n",
    "\n",
    "Exemple : \n",
    "\n",
    "<img src=\"../src/figures/shap.png\" width=1000 height=700 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1af5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Let us have a look on SHAP summary plot\n",
    "samples = val[features].sample(1000)\n",
    "\n",
    "explainer = shap.TreeExplainer(regressor)\n",
    "shap_values = explainer.shap_values(samples, approximate=False, check_additivity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554cbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, samples, alpha=0.5, plot_size=(20, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb166a0d",
   "metadata": {},
   "source": [
    "The main conclusions that we can have following the global estimates of the explanatory factors of energy consumption are as follows:\n",
    "    \n",
    "    - the area of the buildings is the most influential factor on energy consumption. in fact, the larger the area is, the more energy overconsumption is observed\n",
    "    \n",
    "    - buildings for educational use are also the buldings that consume the most energy\n",
    "    \n",
    "    - Very high temperatures and poor air circulation are also a source of overconsumption of energy.\n",
    "    \n",
    "    - the Chilledwater and Steam energy source is the most energy intensive source\n",
    "    \n",
    "The regional effect can be explore by separating data into european an us data to see if effects are same between these two regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a34d4",
   "metadata": {},
   "source": [
    "## To Do\n",
    "\n",
    "1. Fit a regression only on European building\n",
    "2. Explain the main factor of overconsumption with SHAP\n",
    "3. Do same analysis with US building\n",
    "4. Do you remark a difference between the two region according to the main factor of overconsumption ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a29ac7",
   "metadata": {},
   "source": [
    "$\\textbf{Go further ! }$    \n",
    "\n",
    "\n",
    "- [More about Treee feature importance](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)\n",
    "- [More about permutation importance](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
    "- [The technical details about LIME ](https://christophm.github.io/interpretable-ml-book/lime.html)\n",
    "- [A python example of how LIME is used (the LIME github is not the most helpful) ](https://coderzcolumn.com/tutorials/machine-learning/how-to-use-lime-to-understand-sklearn-models-predictions)\n",
    "- [SHAP's GitHub (you'll also find the research paper there)](https://github.com/slundberg/shap)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3207b46d55f06357cbc786d58801af30adee4e85f2fbb9c5029957ffa03d32a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
