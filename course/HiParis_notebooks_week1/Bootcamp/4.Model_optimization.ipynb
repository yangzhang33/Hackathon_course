{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcc2af2",
   "metadata": {},
   "source": [
    "## 4.Model_optimization\n",
    "\n",
    "Authors : Haddam Yacine, Ka Alioune, Renaud Adrien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204a1a2",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <a>\n",
    "    <img src=\"../src/figures/logo-hi-paris-retina.png\" alt=\"Logo\" width=\"280\" height=\"180\">\n",
    "  </a>\n",
    "\n",
    "  <h3 align=\"center\">Data Science Bootcamp</h3>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a72b50",
   "metadata": {},
   "source": [
    "In this lab, we will introduce some useful tools to improve a machine learning model. Sometimes, your machine learning models just donâ€™t work as well as expected. When faced with this situation, what many people do is try different methods more or less at random or follow their guts. It might be : \n",
    "\n",
    "- **Adding more data**\n",
    "- **Improve data quality**\n",
    "- **Feature Selection**\n",
    "- **Search best hyperparameters**\n",
    "- **Trying a new model (or Averaging many model's output)**\n",
    "- **Tweaking some variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f7c16",
   "metadata": {},
   "source": [
    "### Data Path\n",
    "\n",
    "`data_dir` is the path to data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b7f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/jovyan/personal_workspace/bootcamp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5486186",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35f672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, max_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "sys.path.append('../src/notebooks')\n",
    "from utils.utils_optimization import (\n",
    "    select_best_features,\n",
    "    variance_threshold_selector,\n",
    "    feature_importance_selector,\n",
    "    recursive_selection,\n",
    "    F_test_selector\n",
    ")\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298aa091",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eff2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_feather(os.path.join(data_dir, 'model/train.feather'))\n",
    "val = pd.read_feather(os.path.join(data_dir, 'model/val.feather'))\n",
    "test = pd.read_feather(os.path.join(data_dir, 'model/test.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    # 'building_id',\n",
    "    'lat',\n",
    "    'lng',\n",
    "    'square_feet',\n",
    "    'air_temperature',\n",
    "    'dew_temperature',\n",
    "    'precip_depth_1_hr',\n",
    "    'wind_speed',\n",
    "    'sea_level_pressure',\n",
    "    'wind_direction',\n",
    "    'hour',\n",
    "    'weekday',\n",
    "    'month',\n",
    "    'meter_name_chilledwater',\n",
    "    'meter_name_electricity',\n",
    "    'meter_name_hotwater',\n",
    "    'meter_name_steam',\n",
    "    'primary_use_Education',\n",
    "    'primary_use_Entertainment/public assembly',\n",
    "    'primary_use_Healthcare',\n",
    "    'primary_use_Industry',\n",
    "    'primary_use_Lodging/residential',\n",
    "    'primary_use_Office',\n",
    "    'primary_use_Other',\n",
    "    'primary_use_Parking',\n",
    "    'primary_use_Public services',\n",
    "    'primary_use_Services',\n",
    "    'zone_geo_EUROPE',\n",
    "    'zone_geo_US',\n",
    "    'site_id_0',\n",
    "    'site_id_1',\n",
    "    'site_id_2',\n",
    "    'site_id_3',\n",
    "    'site_id_4',\n",
    "    'site_id_5',\n",
    "    'site_id_6',\n",
    "    'site_id_7',\n",
    "    'site_id_9',\n",
    "    'site_id_11',\n",
    "    'site_id_12',\n",
    "    'site_id_13',\n",
    "    'site_id_15',\n",
    "]\n",
    "\n",
    "target = \"meter_reading\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffd6ce1",
   "metadata": {},
   "source": [
    "##  Selecting best features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942aac3f",
   "metadata": {},
   "source": [
    "Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in.\n",
    "\n",
    "Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.\n",
    "\n",
    "$\\textbf{Benefits of performing feature selection}$\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduces Training Time: fewer data points reduce algorithm complexity and algorithms train faster.\n",
    "\n",
    "\n",
    "$\\textbf{Methods}$:\n",
    "\n",
    "- **Linear selection** :  based on [Fisher test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test) (or Chi-squared test), it consist on making one by one linear regression for all variables in the dataset. After regressions, you have to keep variables with low [p-value](https://quantifyinghealth.com/p-value-explanation/).\n",
    "\n",
    "\n",
    "- **Removing features with low variance** ([VarianceThreshold](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)).\n",
    "\n",
    "\n",
    "- **Feature Importance** : delete all variables with low importance according to a model. [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html).\n",
    "\n",
    "\n",
    "- **Recursive Feature Elimination ([RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html))** : starting with all features in the training dataset, fitting the given machine learning algorithm used in the core of the model, ranking features by importance, discarding the least important features, and re-fitting the model. This process is repeated until a specified number of features remains.\n",
    "\n",
    "\n",
    "\n",
    "- **Go further**\n",
    "    - Stepwise\n",
    "    - LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc041d",
   "metadata": {},
   "source": [
    "We can use each of the methods independently with something like:\n",
    "```python\n",
    "variance_threshold_selector(X_train)\n",
    "F_test_selector(X_train, y_train, 0.05)\n",
    "feature_importance_selector(model, X_train, y_train, threshold=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40784ec",
   "metadata": {},
   "source": [
    "But we can apply them all, and gather all the results in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0aed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some thresholds that will be applied to select the features\n",
    "p_value = .05\n",
    "var_threshold = .15\n",
    "feat_impor_threshold = .01\n",
    "\n",
    "# define a model that we can use to select features\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=5,\n",
    "    max_depth=16,\n",
    "    random_state=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# apply all the methods\n",
    "selected_features_df = select_best_features(\n",
    "    train[features], train[target],\n",
    "    p_value,\n",
    "    var_threshold,\n",
    "    model,\n",
    "    feat_impor_threshold,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a495d7",
   "metadata": {},
   "source": [
    "The table indicates, for each method, if the feature was selected. We can analysis the table to select the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83574bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c8e75",
   "metadata": {},
   "source": [
    "## Searching the best hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d8e91",
   "metadata": {},
   "source": [
    "An optimization procedure involves also defining a search space. This can be thought of geometrically as an n-dimensional volume, where each hyperparameter represents a different dimension and the scale of the dimension are the values that the hyperparameter may take on, such as real-valued, integer-valued, or categorical.\n",
    "\n",
    "$\\textbf{Search Space:}$ Volume to be searched where each dimension represents a hyperparameter and each point represents one model configuration.\n",
    "A point in the search space is a vector with a specific value for each hyperparameter value. The goal of the optimization procedure is to find a vector that results in the best performance of the model after learning, such as maximum accuracy or minimum error.\n",
    "\n",
    "A range of different optimization algorithms may be used, although two of the simplest and most common methods are random search and grid search.\n",
    "\n",
    "$\\textbf{Random Search :}$ Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.\n",
    "\n",
    "$\\textbf{Grid Search : }$ Define a search space as a grid of hyperparameter values and evaluate every position in the grid.\n",
    "\n",
    "Grid search is great for spot-checking combinations that are known to perform well generally. Random search is great for discovery and getting hyperparameter combinations that you would not have guessed intuitively, although it often requires more time to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48278bd6",
   "metadata": {},
   "source": [
    "We install a additional package for GridSearch. Just execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install hypopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1036fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypopt import GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb89eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that we will use in the grid search\n",
    "model = RandomForestRegressor(random_state=0, n_jobs=-1, max_depth=16)\n",
    "\n",
    "# Define the grid of parameters that will be searched\n",
    "param_grid = {\n",
    "    'n_estimators': [2, 10],\n",
    "    'max_depth': [8, 16]\n",
    "}\n",
    "\n",
    "# Create the GridSearch object with the model and the grid of parameters\n",
    "grid_search = GridSearch(model=model, param_grid=param_grid, seed=0, parallelize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b615be3",
   "metadata": {},
   "source": [
    "Lets fit the GridSearch.\n",
    "For each set of parameters, we:\n",
    "- fit the model on the train set\n",
    "- evaluate the performances on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = grid_search.fit(train[features], train[target], val[features], val[target], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee7a421",
   "metadata": {},
   "source": [
    "We can access the performances of the model for each set of parameters. Here, the metric used is the R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fab032",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.get_param_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404dcd9",
   "metadata": {},
   "source": [
    "Finally, we compute our MAE and MSE metrics on the validation set for the best model found with the grid search.  \n",
    "The best model can be accessed with `grid_search.best_estimator_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf77dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute our MAE and MSE metrics on the validation set\n",
    "def print_errors(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    print(f'MAE : {mean_absolute_error(y, y_pred):.0f}')\n",
    "    print(f'MSE : {mean_squared_error(y, y_pred):.0f}')\n",
    "    print(f'MAX : {max_error(y, y_pred):.0f}')\n",
    "\n",
    "\n",
    "print('Erros on Validation set : ')\n",
    "print_errors(grid_search.best_estimator_, val[features], val[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cff8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute our MAE and MSE metrics on the test set\n",
    "print('Erros on Test set : ')\n",
    "print_errors(grid_search.best_estimator_, test[features], test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9013c3",
   "metadata": {},
   "source": [
    "# It's your turn\n",
    "\n",
    "Using what we learned so far, try to:\n",
    "- improve your model\n",
    "- create some insightful visualizations\n",
    "- understand better your model performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a1f57",
   "metadata": {},
   "source": [
    "$\\textbf{Go further ! }$\n",
    "- Hastie T., Tibshirani R., Friedman J., Â« [The elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) - Data Mining, Inference and Prediction Â», pringer, 2009."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3207b46d55f06357cbc786d58801af30adee4e85f2fbb9c5029957ffa03d32a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
